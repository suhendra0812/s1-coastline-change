{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from datacube import Datacube\n",
    "from dea_tools.spatial import subpixel_contours\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyTMD\n",
    "from scipy.ndimage import binary_fill_holes, gaussian_filter1d, uniform_filter, variance\n",
    "from skimage import img_as_ubyte\n",
    "from skimage.filters.thresholding import threshold_local, threshold_otsu\n",
    "from skimage.morphology import remove_small_objects\n",
    "from shapely.geometry import LineString, MultiLineString\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = Datacube(app=__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_path = Path(\"../region/coastal_grids.geojson\")\n",
    "point_path = Path(\"../region/coastal_points.geojson\")\n",
    "\n",
    "region_gdf = gpd.read_file(region_path)\n",
    "point_gdf = gpd.read_file(point_path)\n",
    "\n",
    "filter_region_gdf = region_gdf.query(\"province == 'BALI'\")\n",
    "centroid = filter_region_gdf.unary_union.centroid\n",
    "\n",
    "m = region_gdf.explore(\n",
    "    location=[centroid.y, centroid.x],\n",
    "    zoom_start=9,\n",
    "    style_kwds={\"fillOpacity\": 0, \"color\": \"red\", \"linewidth\": 1}\n",
    ")\n",
    "\n",
    "folium.GeoJson(point_gdf.to_json()).add_to(m)\n",
    "\n",
    "for i, row in region_gdf.iterrows():\n",
    "    centroid = row.geometry.centroid\n",
    "    folium.Marker(\n",
    "        location=[centroid.y, centroid.x],\n",
    "        icon=folium.DivIcon(\n",
    "            html=f\"<div style='font-size: 12px'>{i+1}</div>\"\n",
    "        )\n",
    "    ).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_id = 715\n",
    "\n",
    "selected_region_gdf = region_gdf.loc[[region_id-1]]\n",
    "selected_point_gdf = point_gdf.loc[[region_id-1]]\n",
    "\n",
    "m = selected_region_gdf.explore(style_kwds={\"fillOpacity\": 0, \"color\": \"red\"})\n",
    "folium.GeoJson(selected_point_gdf.to_json()).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, ymin, xmax, ymax = selected_region_gdf.total_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = {\n",
    "    \"time\": (\"2015-01-01\", \"2022-05-31\"),\n",
    "    \"longitude\": (xmin, xmax),\n",
    "    \"latitude\": (ymin, ymax),\n",
    "    # \"dask_chunks\": {\"time\": 1, \"x\": 1024, \"y\": 1024},\n",
    "}\n",
    "\n",
    "ds = dc.load(product=\"s1_iw_gamma0_rtc_vh\", **search_query)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_list = ds.time.values\n",
    "time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_constants(\n",
    "    lon: np.array, lat: np.array, model: pyTMD.model\n",
    ") -> tuple:\n",
    "\n",
    "    print(\"Extracting constants...\")\n",
    "\n",
    "    # get amplitude and phase\n",
    "    amp, ph = pyTMD.extract_FES_constants(\n",
    "        np.atleast_1d(lon),\n",
    "        np.atleast_1d(lat),\n",
    "        model.model_file,\n",
    "        TYPE=model.type,\n",
    "        VERSION=model.version,\n",
    "        METHOD=\"spline\",\n",
    "        EXTRAPOLATE=True,\n",
    "        SCALE=model.scale,\n",
    "        GZIP=model.compressed,\n",
    "    )\n",
    "\n",
    "    return amp, ph\n",
    "\n",
    "\n",
    "def model_tide_prediction(lon: np.array, lat: np.array, date_list: np.array, model_dir: Path) -> np.array:\n",
    "    \n",
    "    print(\"Tide prediction\")\n",
    "\n",
    "    # convert list of datetime\n",
    "    tide_time = pyTMD.time.convert_datetime(date_list)\n",
    "\n",
    "    # define model directory and initialize model based on model format\n",
    "    model = pyTMD.model(model_dir, format=\"FES\", compressed=False).elevation(\"FES2014\")\n",
    "\n",
    "    # get tide constants (amplitude and phase) and it will take a while\n",
    "    amp, ph = get_constants(lon, lat, model)\n",
    "\n",
    "    # extract model constituent\n",
    "    c = model.constituents\n",
    "\n",
    "    # calculate delta time\n",
    "    delta_file = pyTMD.utilities.get_data_path([\"data\", \"merged_deltat.data\"])\n",
    "    DELTAT = pyTMD.calc_delta_time(delta_file, tide_time)\n",
    "\n",
    "    # calculate complex phase in radians for Euler's\n",
    "    cph = -1j * ph * np.pi / 180.0\n",
    "\n",
    "    # calculate constituent oscillation\n",
    "    hc = amp * np.exp(cph)\n",
    "\n",
    "    # predict tidal time series\n",
    "    TIDE = pyTMD.predict_tidal_ts(\n",
    "        tide_time, hc, c, DELTAT=DELTAT, CORRECTIONS=model.format\n",
    "    )\n",
    "\n",
    "    # infer minor corrections\n",
    "    MINOR = pyTMD.infer_minor_corrections(\n",
    "        tide_time, hc, c, DELTAT=DELTAT, CORRECTIONS=model.format\n",
    "    )\n",
    "\n",
    "    # calculate tide with minor correction\n",
    "    TIDE.data[:] += MINOR.data[:]\n",
    "    \n",
    "    print(\"Done\")\n",
    "\n",
    "    return TIDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = selected_point_gdf.unary_union.centroid.x\n",
    "y = selected_point_gdf.unary_union.centroid.y\n",
    "\n",
    "lons = np.repeat(x, len(time_list))\n",
    "lats = np.repeat(y, len(time_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = Path(\"../datasets/tide/model\")\n",
    "tide_list = model_tide_prediction(lons, lats, time_list, model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt = np.min(tide_list)\n",
    "ht = np.max(tide_list)\n",
    "mean = np.mean(tide_list)\n",
    "\n",
    "print(f\"Low tide: {lt}\")\n",
    "print(f\"High tide: {ht}\")\n",
    "print(f\"Mean tide: {mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tide_ds = xr.DataArray(tide_list, coords=[ds.time], dims=[\"time\"])\n",
    "tide_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"tide\"] = tide_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tide(group_ds, tide):\n",
    "    ds_list = []\n",
    "    for _, group in group_ds:\n",
    "        ds = group.isel(time=np.argsort(np.abs(group.tide.values - tide))[0])\n",
    "        ds_list.append(ds)\n",
    "    all_ds = xr.concat(ds_list, dim=\"time\")\n",
    "    return all_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_ds = ds.groupby(\"time.year\")\n",
    "ht_ds = filter_tide(group_ds, ht)\n",
    "lt_ds = filter_tide(group_ds, lt)\n",
    "mean_ds = filter_tide(group_ds, mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_scale(img):\n",
    "    db_output = 10 * np.log10(img)\n",
    "    return db_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"vh_db\"] = ds.vh.groupby(\"time\").apply(db_scale)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=True, figsize=(15, 10))\n",
    "\n",
    "ds.isel(time=-1).vh.plot(cmap=\"gray\", robust=True, ax=axes.flatten()[0])\n",
    "ds.isel(time=-1).vh_db.plot(cmap=\"gray\", robust=True, ax=axes.flatten()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lee_filter(img, size):\n",
    "    img_mean = uniform_filter(img, size)\n",
    "    img_sqr_mean = uniform_filter(img**2, size)\n",
    "    img_variance = img_sqr_mean - img_mean**2\n",
    "\n",
    "    overall_variance = variance(img)\n",
    "\n",
    "    img_weights = img_variance / (img_variance + overall_variance)\n",
    "    img_output = img_mean + img_weights * (img - img_mean)\n",
    "    return img_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"vh_db_filter\"] = (\n",
    "    ds.vh_db\n",
    "    .groupby(\"time\")\n",
    "    .apply(lambda img: xr.apply_ufunc(\n",
    "            lee_filter,\n",
    "            img,\n",
    "            kwargs={\"size\": 5},\n",
    "            # dask=\"parallelized\",\n",
    "            # dask_gufunc_kwargs={\"allow_rechunk\": True}\n",
    "        )\n",
    "    )\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=True, figsize=(15, 10))\n",
    "\n",
    "ds.isel(time=-1).vh_db.plot(cmap=\"gray\", robust=True, ax=axes.flatten()[0])\n",
    "ds.isel(time=-1).vh_db_filter.plot(cmap=\"gray\", robust=True, ax=axes.flatten()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def local_binary(img: np.ndarray, *args, **kwargs) -> np.ndarray:\n",
    "    img = img_as_ubyte(img)\n",
    "    threshold = threshold_local(img, *args, **kwargs)\n",
    "    binary = img >= threshold\n",
    "    binary = binary_fill_holes(binary)\n",
    "    binary = remove_small_objects(binary)\n",
    "    return binary.astype(np.uint8)\n",
    "\n",
    "\n",
    "def otsu_binary(img: np.ndarray) -> np.ndarray:\n",
    "    img = img_as_ubyte(img)\n",
    "    threshold = threshold_otsu(img)\n",
    "    binary = img >= threshold\n",
    "    binary = binary_fill_holes(binary)\n",
    "    binary = remove_small_objects(binary)\n",
    "    return binary.astype(np.uint8)\n",
    "\n",
    "\n",
    "def smooth_linestring(linestring, smooth_sigma):\n",
    "    \"\"\"\n",
    "    Uses a gauss filter to smooth out the LineString coordinates.\n",
    "    \"\"\"\n",
    "    smooth_x = np.array(gaussian_filter1d(linestring.xy[0], smooth_sigma))\n",
    "    smooth_y = np.array(gaussian_filter1d(linestring.xy[1], smooth_sigma))\n",
    "    smoothed_coords = np.hstack((smooth_x, smooth_y))\n",
    "    smoothed_coords = zip(smooth_x, smooth_y)\n",
    "    linestring_smoothed = LineString(smoothed_coords)\n",
    "    return linestring_smoothed\n",
    "\n",
    "\n",
    "def create_transects(line, space, length, crs):\n",
    "    # Profile spacing. The distance at which to space the perpendicular profiles\n",
    "    # In the same units as the original shapefile (e.g. metres)\n",
    "    space = space\n",
    "\n",
    "    # Length of cross-sections to calculate either side of central line\n",
    "    # i.e. the total length will be twice the value entered here.\n",
    "    # In the same co-ordinates as the original shapefile\n",
    "    length = length\n",
    "\n",
    "    # Define a schema for the output features. Add a new field called 'Dist'\n",
    "    # to uniquely identify each profile\n",
    "\n",
    "    transect_list = []\n",
    "\n",
    "    # Calculate the number of profiles to generate\n",
    "    n_prof = int(line.length / space)\n",
    "\n",
    "    # Start iterating along the line\n",
    "    for prof in range(1, n_prof + 1):\n",
    "        # Get the start, mid and end points for this segment\n",
    "        seg_st = line.interpolate((prof - 1) * space)\n",
    "        seg_mid = line.interpolate((prof - 0.5) * space)\n",
    "        seg_end = line.interpolate(prof * space)\n",
    "\n",
    "        # Get a displacement vector for this segment\n",
    "        vec = np.array(\n",
    "            [\n",
    "                [\n",
    "                    seg_end.x - seg_st.x,\n",
    "                ],\n",
    "                [\n",
    "                    seg_end.y - seg_st.y,\n",
    "                ],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Rotate the vector 90 deg clockwise and 90 deg counter clockwise\n",
    "        rot_anti = np.array([[0, -1], [1, 0]])\n",
    "        rot_clock = np.array([[0, 1], [-1, 0]])\n",
    "        vec_anti = np.dot(rot_anti, vec)\n",
    "        vec_clock = np.dot(rot_clock, vec)\n",
    "\n",
    "        # Normalise the perpendicular vectors\n",
    "        len_anti = ((vec_anti**2).sum()) ** 0.5\n",
    "        vec_anti = vec_anti / len_anti\n",
    "        len_clock = ((vec_clock**2).sum()) ** 0.5\n",
    "        vec_clock = vec_clock / len_clock\n",
    "\n",
    "        # Scale them up to the profile length\n",
    "        vec_anti = vec_anti * length\n",
    "        vec_clock = vec_clock * length\n",
    "\n",
    "        # Calculate displacements from midpoint\n",
    "        prof_st = (seg_mid.x + float(vec_clock[0]), seg_mid.y + float(vec_clock[1]))\n",
    "        prof_end = (seg_mid.x + float(vec_anti[0]), seg_mid.y + float(vec_anti[1]))\n",
    "\n",
    "        distance = (prof - 0.5) * space\n",
    "        transect = LineString([prof_end, prof_st])\n",
    "\n",
    "        gdf = gpd.GeoDataFrame({\"distance\": [distance]}, geometry=[transect])\n",
    "\n",
    "        transect_list.append(gdf)\n",
    "\n",
    "    transect_gdf = pd.concat(transect_list, ignore_index=True)\n",
    "    transect_gdf.crs = crs\n",
    "\n",
    "    return transect_gdf\n",
    "\n",
    "\n",
    "def transect_analysis(line_gdf, transect_gdf, time_column, reverse=False):\n",
    "    line_gdf[time_column] = pd.to_datetime(line_gdf[time_column])\n",
    "    line_gdf[\"time_idx\"], _ = pd.factorize(line_gdf[time_column])\n",
    "\n",
    "    line_gdf.sort_values(by=time_column, inplace=True, ignore_index=True)\n",
    "    transect_gdf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    analysis_list = []\n",
    "\n",
    "    for i, transect in transect_gdf.iterrows():\n",
    "        start, end = transect.geometry.boundary.geoms\n",
    "        if reverse:\n",
    "            start = end\n",
    "        if any(line_gdf.geometry.intersects(transect.geometry)):\n",
    "            intersect_gdf = line_gdf.copy()\n",
    "            intersect_gdf.geometry = intersect_gdf.geometry.intersection(\n",
    "                transect.geometry\n",
    "            )\n",
    "            geom_types = [geom.geom_type for geom in intersect_gdf.geometry]\n",
    "            if geom_types.count(\"Point\") == len(intersect_gdf):\n",
    "                oldest_date = intersect_gdf.iloc[0][time_column]\n",
    "                oldest_geom = intersect_gdf.iloc[0][\"geometry\"]\n",
    "                oldest_distance = oldest_geom.distance(start)\n",
    "\n",
    "                analysis_data = {\"name\": [i]}\n",
    "\n",
    "                for j in range(len(intersect_gdf)):\n",
    "                    intersect = intersect_gdf.iloc[j]\n",
    "                    test_date = intersect[time_column]\n",
    "                    time_str = test_date.strftime(\"%Y%m%d\")\n",
    "                    time_idx = intersect[\"time_idx\"]\n",
    "\n",
    "                    if j > 0:\n",
    "                        distance = intersect.geometry.distance(start)\n",
    "                        change = distance - oldest_distance\n",
    "                        rate = change / (test_date - oldest_date).days / 365\n",
    "                    else:\n",
    "                        distance = oldest_distance\n",
    "                        change = 0\n",
    "                        rate = 0\n",
    "\n",
    "                    analysis_data[f\"distance_{time_str}\"] = [distance]\n",
    "                    analysis_data[f\"change_{time_str}\"] = [change]\n",
    "                    analysis_data[f\"rate_{time_str}\"] = [rate]\n",
    "\n",
    "                analysis_geom = LineString(intersect_gdf.geometry)\n",
    "\n",
    "                analysis_gdf = gpd.GeoDataFrame(analysis_data, geometry=[analysis_geom])\n",
    "\n",
    "                distance_columns = analysis_gdf.columns[\n",
    "                    analysis_gdf.columns.str.contains(\"distance\")\n",
    "                ]\n",
    "                analysis_gdf[\"mean_distance\"] = analysis_gdf[distance_columns].mean(\n",
    "                    axis=1\n",
    "                )\n",
    "\n",
    "                change_columns = analysis_gdf.columns[\n",
    "                    analysis_gdf.columns.str.contains(\"change\")\n",
    "                ]\n",
    "                analysis_gdf[\"mean_change\"] = analysis_gdf[change_columns].mean(axis=1)\n",
    "\n",
    "                rate_columns = analysis_gdf.columns[\n",
    "                    analysis_gdf.columns.str.contains(\"rate\")\n",
    "                ]\n",
    "                analysis_gdf[\"mean_rate\"] = analysis_gdf[rate_columns].mean(axis=1)\n",
    "\n",
    "                analysis_list.append(analysis_gdf)\n",
    "    \n",
    "    transect_analysis_gdf = pd.concat(analysis_list, ignore_index=True)\n",
    "    transect_analysis_gdf.crs = line_gdf.crs\n",
    "\n",
    "    return transect_analysis_gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.where(ds != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_ds = ds.isel(time=-1).load()\n",
    "selected_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_ds.vh.plot(cmap=\"gray\", robust=True, size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ds = ds.dropna(dim=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_ds.time.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ds[\"filtered_vh\"] = filtered_ds[\"vh\"].groupby(\"time\").apply(lee_filter, size=7)\n",
    "filtered_ds[\"filtered_vh_db\"] = filtered_ds[\"filtered_vh\"].groupby(\"time\").apply(convert_raster)\n",
    "filtered_ds[\"filtered_vh_db_binary\"] = filtered_ds[\"filtered_vh_db\"].groupby(\"time\").apply(otsu_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(f\"./output/bali/{ID:04d}\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_path = output_dir.joinpath(f\"{ID:04d}_s1_filtered_vh_db.tif\")\n",
    "filtered_ds.filtered_vh_db.rio.to_raster(raster_path, compress=\"lzw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ds.filtered_vh_db_binary.isel(time=slice(0, 4)).load().plot(cmap=\"Greys_r\", robust=True, size=5, col=\"time\", col_wrap=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coastline_gdf = subpixel_contours(\n",
    "    da=filtered_ds.filtered_vh_db_binary,\n",
    "    affine=filtered_ds.filtered_vh_db_binary.rio.transform(),\n",
    "    crs=filtered_ds.filtered_vh_db_binary.rio.crs,\n",
    "    min_vertices=100\n",
    ")\n",
    "\n",
    "smooth_lines = []\n",
    "for line in coastline_gdf.geometry:\n",
    "    if line.geom_type == \"LineString\":\n",
    "        smooth_line = smooth_linestring(line, 5)\n",
    "    else:\n",
    "        smooth_line = MultiLineString([smooth_linestring(subline, 5) for subline in line.geoms])\n",
    "    smooth_lines.append(smooth_line)\n",
    "\n",
    "coastline_gdf[\"geometry\"] = smooth_lines\n",
    "coastline_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coastline_path = output_dir.joinpath(f\"{ID:04d}_s1_coastlines.geojson\")\n",
    "coastline_gdf.to_file(coastline_path, driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = coastline_gdf.geometry.iloc[0]\n",
    "transect_gdf = create_transects(baseline, 500, 100, crs=coastline_gdf.crs)\n",
    "transect_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transect_path = output_dir.joinpath(f\"{ID:04d}_s1_transects.geojson\")\n",
    "transect_gdf.to_file(transect_path, driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transect_analysis_gdf = transect_analysis(coastline_gdf, transect_gdf, \"time\", reverse=True)\n",
    "transect_analysis_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transect_analysis_path = output_dir.joinpath(f\"{ID:04d}_s1_transect_analysis.geojson\")\n",
    "transect_analysis_gdf.to_file(transect_analysis_path, driver=\"GeoJSON\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0b847be34aecb558b376b6ab85408bacbc974dcb60f21f7ac2ed6ffce76a82dd"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
